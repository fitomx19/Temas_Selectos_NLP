{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming y Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name util",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/adolfohuerta/Desktop/Temas_Selectos_NLP/Practica 5.1/LematizacionSteaming.ipynb Celda 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adolfohuerta/Desktop/Temas_Selectos_NLP/Practica%205.1/LematizacionSteaming.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcli\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglossary\u001b[39;00m \u001b[39mimport\u001b[39;00m explain  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/pipeline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mattributeruler\u001b[39;00m \u001b[39mimport\u001b[39;00m AttributeRuler\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdep_parser\u001b[39;00m \u001b[39mimport\u001b[39;00m DependencyParser\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39medit_tree_lemmatizer\u001b[39;00m \u001b[39mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/pipeline/attributeruler.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrsly\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpipe\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipe\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m Errors\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Example\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/pipeline/pipe.pyx:1\u001b[0m, in \u001b[0;36minit spacy.pipeline.pipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtoken\u001b[39;00m \u001b[39mimport\u001b[39;00m Token\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mspan\u001b[39;00m \u001b[39mimport\u001b[39;00m Span\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/doc.pyx:34\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name util"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]    \n",
    "    return lexical_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la función de normalización.\n",
    "lemmas = normalize(\"\"\"texto de prueba para la lematización de palabras en español. \n",
    "                    Esta es una prueba de la función de normalización de palabras.\"\"\")\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanishstemmer = SnowballStemmer(\"spanish\")\n",
    "text = \"\"\"texto de prueba para la lematización de palabras en español. \n",
    "                    Esta es una prueba de la función de normalización de palabras.\"\"\"\n",
    " \n",
    "tokens = text.split() \n",
    "stems = [spanishstemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
