{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traduccion automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SMT ha sido el enfoque dominante para la TA durante muchos años, especialmente desde mediados de la década de 1990 hasta aproximadamente 2010.\n",
    "\n",
    "Sin embargo, con el advenimiento de los modelos de aprendizaje profundo y las redes neuronales, la Traduccion Automatica con Redes Neuronales la ha reemplazado\n",
    "\n",
    "Corpus Paralelo: La SMT requier un corpues paralelo, que son conjuntos de textos en dos (o más ) idiomas que son traducciones directas entre sí. A partir de estos corpora, se extraen patrones y se contruyen modelos estadísticos para la traducción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo basado en frases:__ En lugar de traducr palabra po palabra, la SMT a menudo opera a nivel de \"frases\" (Segmentos de texto que pueden ser de una palabra hasta varias palabras de longitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo de lenguaje:__ Ademas del modelo de traducción ,la SMT utiliza modelos de lenguaje para el idioma objetivo.\n",
    "\n",
    "Estos modelos ofrecen probabilidades sobre secuencias de palbras en el idioma objetivo, lo que ayuda a garantizar que las traducciones sean fluidas y naturales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reordenamiento__: Dado que diferentes idiomas tiene diferentes estructuras gramaticales y órdenes de palabras, la TAE incluye modelos de reordenamiento que intentan predecir cómo las palabras y frases deben reoganizarse en la traducción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decodificación__: Durante la fase de traducción, un decodificador busca la traducción más probable utilizando el modelo de traducción y el modelo de lenguaje.\n",
    "\n",
    "__Técnicas de Aprendizaje Automático__: Algunas implementaciones de SMT utilizan técnicas de aprendijzaje automático para mejorar la precisión de la traducción, ajusntando los modelos basados en el feedback o en nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, herramientas como __Moses__ son utilizadas para implemntar TAE con muchas más características y optimizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ingles = ['I am a boy', 'I am a girl' , 'this is a pen']\n",
    "corpus_espanol = ['Soy un niño', 'Soy una niña', 'esto es una pluma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'I': 2, 'am': 2, 'a': 3, 'boy': 1, 'girl': 1, 'this': 1, 'is': 1, 'pen': 1})\n",
      "defaultdict(<class 'int'>, {'Soy': 2, 'un': 1, 'niño': 1, 'una': 2, 'niña': 1, 'esto': 1, 'es': 1, 'pluma': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocab_ingles = defaultdict(int)\n",
    "vocab_espanol = defaultdict(int)\n",
    "\n",
    "for sentence in corpus_ingles:\n",
    "    for word in sentence.split():\n",
    "        vocab_ingles[word] += 1\n",
    "\n",
    "for sentence in corpus_espanol:\n",
    "    for word in sentence.split():\n",
    "        vocab_espanol[word] += 1\n",
    "\n",
    "\n",
    "print(vocab_ingles)\n",
    "print(vocab_espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 'Soy', 'am': 'una', 'a': 'una', 'this': 'esto', 'is': 'es', 'pen': 'pluma'}\n"
     ]
    }
   ],
   "source": [
    "#creacion de modelo de traduccion basico\n",
    "\n",
    "diccionario_traduccion = {}\n",
    "for sent_ingles, sent_espanol in zip (corpus_ingles, corpus_espanol):\n",
    "    words_ingles = sent_ingles.split()\n",
    "    words_espanol = sent_espanol.split()\n",
    "    for w_ingles, w_espanol in zip(words_ingles, words_espanol):\n",
    "        diccionario_traduccion[w_ingles] = w_espanol\n",
    "\n",
    "print(diccionario_traduccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soy una una boy\n"
     ]
    }
   ],
   "source": [
    "def traducir(oracion):\n",
    "    return ' '.join([diccionario_traduccion.get(word, word) for word in oracion.split()])\n",
    "\n",
    "oracion_test = \"I am a boy\"\n",
    "print(traducir(oracion_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduccion automatica Neuronal\n",
    "\n",
    "La traduccion atutomatica neuronal (NMT) es un enfoque en el campo de la TA que utiliza redes neuronales profundas, especialmente las arquitecturas de redes neuronales recurrentes (RNN) y de atencion (comno modelo Tranfsmorder), para traducir texto de un idioma a otro.\n",
    "\n",
    "Representa una de las mas significativas innovaciones en la TA en la ultima decada y ha superado a los enfoques tradicionales, como la SMT, en terminos de calidad y fluidez de las traducciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo de secuencia a secuencia (Seq2Seq):__ Una de las arquitecturas más populares en TAN es el modelo Seq2Seq, que utiliza una RNN en el \"encoder\" para procesar la secuencia de entrada y otra RNN en el \"decoder\" para generar la secuencia de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo Transformer__: Introducido en el paper \"Attention is All you need\" por Vaswani et al., el modelo Transformer se ha convertido en la base de muchas sistemas de TAN modernos.\n",
    "\n",
    "Utiliza múltiples capas de atención para procesar el texto de entrada y generar la traducción. El transformer ha sido fundamental en la creación de modelos de lenguaje preentrenados masivos como BERT,GPT y otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Embeddings de Palabras__ : La TAN utiliza embeddings (incrustraciones vectoriales) para representar palabras o subpalabras, lo que permite que las redes neuronales trabajen con representaciones densas y continuas de las palbras en lugar de representaciones dispersas y categoróricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Subpalabras y Tokenización:__ En lugar de trabajar con palabras completas, muchos sistemas de TAN itilizan técnicas como BPE (Byte-Pair Encoding) para dividir palabras en subpalabras\n",
    "\n",
    "\n",
    "\n",
    "__Entrenamiento en Grandes Corpora:__ Al igual que las TAE, la TAN se beneficia del entrenaimiento en grandes corpora paralelos.\n",
    "\n",
    "Sin embargo, las redes neuronales requieren cantidades significativamente mayores de datos y potencia computacional para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U transformers\n",
    "\n",
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.11/site-packages (0.1.99)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coffee cat went to sleep under the couch for coffee.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def traducir(texto):\n",
    "    modelo_nombre = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "    modelo = MarianMTModel.from_pretrained(modelo_nombre)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(modelo_nombre)\n",
    "\n",
    "    # Tokenizar el texto en español\n",
    "    texto_tokenizado = tokenizer.encode(texto, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Traducir el texto tokenizado al inglés\n",
    "    traduccion_ids = modelo.generate(texto_tokenizado, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    traduccion = tokenizer.decode(traduccion_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return traduccion\n",
    "\n",
    "oracion_test = \"El gato cafe se fue a dormir bajo el sofa  a tomar cafe\"\n",
    "print(traducir(oracion_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
